#### general settings
# name: ediffsr_train_farm_multiband_plus_NDVI_Lastlayer_UNO_High_High #(12,12)
# name: ediffsr_train_farm_multiband_plus_NDVI_UNO_using_conv_instead_linear #it helps to store position information
name: ediffsr_train_AID_DifGaussian_without_swin
use_tb_logger: true
model: denoising
distortion: sr
gpu_ids: [0]

sde:
  max_sigma: 50
  T: 100
  schedule: cosine # linear, cosine
  eps: 0.005

degradation: # for some synthetic dataset that only have GTs
  # for denoising
  sigma: 25
  noise_type: G # Gaussian noise: G

  # for super-resolution
  scale: 4
  
#### datasets
datasets:
  train:
    optimizer: AdamW # Adam, AdamW, Lion
    name: Train_Dataset
    mode: LQGT
    # dataroot_GT: /workspace/AID_split_matched/HR/train
    # dataroot_LQ: /workspace/AID_split_matched/LR/train
    dataroot_GT: ~/research/EDiffSR/AID_split_matched/HR/train
    dataroot_LQ: ~/research/EDiffSR/AID_split_matched/LR/train

    use_shuffle: true
    n_workers: 8  # per GPU
    batch_size: 4
    GT_size: 256
    LR_size: 64
    use_flip: true
    use_rot: true
    use_noise: False
    use_bright: False
    use_blur: False
    color: RGB
    img_channel: 3 #add my selelf
  val:
    name: Val_Dataset
    mode: LQGT
    # dataroot_GT: /workspace/AID_split_matched/HR/val
    # dataroot_LQ: /workspace/AID_split_matched/LR/val
    dataroot_GT: ~/research/EDiffSR/AID_split_matched/HR/val
    dataroot_LQ: ~/research/EDiffSR/AID_split_matched/LR/val
    color: RGB
    img_channel: 3 #add my selelf


#### network structures
network_G:
  which_model_G: ConditionalNAFNet
  setting:
    width: 64
    enc_blk_nums: [14, 1, 1, 1]
    middle_blk_num: 1
    dec_blk_nums: [1, 1, 1, 1]
    img_channel: 3
  uno_in_width: 3
  uno_width: 48
  uno_out_width: 3
  use_uno: True

  uno_setting:
    trunc_mode_stages: ['LL-LH', 'LL-LH', 'LL-LH', 'LL-LH']
    use_sobel_stages: [0, 0, 0, 0]
    patch_based_stages: [1, 1, 1, 1]
    patch_size_stages: [16, 16, 16, 16]
    factorize_mode_stages: [null, null, 'dep-sep', 'dep-sep']
    use_attn_stages: [0, 0, 0, 1]
    simple_propagate_stages: [0, 0, 0, 0]
    use_swin_stages: [False, False, False, False]
    window_size_stages: [0, 0, 0, 0]
    type_grid: linear
    use_fno_only: True
    skip_type: add

#### path
path:
  pretrain_model_G: ~
  strict_load: true
  # strict_load: false
  # resume_state: /home/yuki/research/EDiffSR/experiments/yuki/ediffsr_train_AID_UNO_Galerkin/training_state/135000.state
  resume_state: ~

#### training settings: learning rate scheme, loss
train:
  optimizer: AdamW # Adam, AdamW, Lion
  lr_G: !!float 4e-5
  lr_scheme: TrueCosineAnnealingLR
  beta1: 0.9
  beta2: 0.99
  niter: 315000
  # niter: 70000  # 60000
  # niter: 4
  warmup_iter: -1  # no warm up
  lr_steps: [100000, 200000, 300000]
  lr_gamma: 0.5
  eta_min: !!float 1e-7

  uno_optimizer: AdamW
  lr_uno: !!float 1e-4
  uno_weight_decay: !!float 1e-3
  uno_lr_steps: [100000, 200000, 300000]
  uno_lr_gamma: 0.5
  uno_beta1: 0.9
  uno_beta2: 0.99
  uno_eta_min: !!float 1e-7
  uno_inwidth: 3
  uno_width: 32
  uno_factor: 0.75
  # UNO + uno_factor: 0.5

  galerkin_optimizer: AdamW
  lr_galerkin: !!float 1e-4
  galerkin_weight_decay: !!float 1e-3
  galerkin_lr_steps: [100000, 200000, 300000]
  galerkin_lr_gamma: 0.5
  galerkin_beta1: 0.9
  galerkin_beta2: 0.99
  uno_eta_min: !!float 1e-7

  # criterion
  is_weighted: False
  loss_type: l1
  weight: 1.0
  edge_weight: 0.3

  manual_seed: 0
  val_freq: !!float 5e3 # 5e3

#### logger
logger:
  print_freq: 100
  save_checkpoint_freq: !!float 5e3
